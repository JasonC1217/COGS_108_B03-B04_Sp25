{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ed4f30",
   "metadata": {},
   "source": [
    "## Week 8 Demo: Text\n",
    "#### This demo is adapted from D7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ef0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import nltk package \n",
    "# NLTK provides support for a wide variety of text processing tasks: \n",
    "# tokenization, stemming, proper name identification, part of speech identification, etc. \n",
    "#   PennTreeBank word tokenizer \n",
    "#   English language stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# scikit-learn imports\n",
    "#   TF-IDF Vectorizer that first removes widely used words in the dataset and then transforms test data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# import re for regular expression\n",
    "import re\n",
    "\n",
    "## seaborn for plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.2, style=\"white\")\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# set plotting size parameter\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "# improve resolution\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74be15",
   "metadata": {},
   "source": [
    "Now let's **import the `inaugural` dataset from `nltk.corpus`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ff550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d50f66",
   "metadata": {},
   "source": [
    "You should be able to see all the txt files in the dataset, but here we just want to display the first 5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa865153",
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed83fc",
   "metadata": {},
   "source": [
    "Let's then extract the years for each speech and save the years in a list called `years`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "for fileid in inaugural.fileids():\n",
    "    years.append(fileid[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5bddc9",
   "metadata": {},
   "source": [
    "Let's check if we have successfully extracted the years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "years[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b08eaf",
   "metadata": {},
   "source": [
    "Let's take a look at one of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafd80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Washington's Second Inaugural Address\n",
    "inaugural.raw('1793-Washington.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e85511",
   "metadata": {},
   "source": [
    "You'll notice that there are some new line characters, as well as a colon, some commas, some periods. We're really only interested in the words though for TF-IDF, so let's remove all punctuation. **Write code that returns a list (`text`), where each element in the list includes the text as above, but with:\n",
    "- punctuation removed \n",
    "- each word separated by a space\n",
    "- all words are lower case (i.e. \"Constitution\" should be \"constitution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537e0aa",
   "metadata": {},
   "source": [
    "Here's a cheatsheet for regular expression: https://cheatography.com/davechild/cheat-sheets/regular-expressions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d2801-d2cd-4733-8205-8db75c1b39bf",
   "metadata": {},
   "source": [
    "#### Processing Inaugural Address Texts — Markdown Table\n",
    "\n",
    "| **Step** | **Code Snippet** | **Explanation** |\n",
    "|----------|------------------|-----------------|\n",
    "| **1. Load Raw Texts** | `inaugural.raw(file_id)` | For each file ID in the NLTK inaugural corpus, retrieve the full raw text of the speech. |\n",
    "| **2. List Comprehension** | `[inaugural.raw(file_id) for file_id in inaugural.fileids()]` | Iterates through all file IDs in the corpus to collect a list of raw speech texts. |\n",
    "| **3. Clean Text** | `re.sub(r'[^A-Za-z0-9]+', ' ', x)` | Uses a regular expression to replace any non-alphanumeric characters (including punctuation) with spaces. |\n",
    "| **4. Cleaned List Comprehension** | `[re.sub(r'[^A-Za-z0-9]+', ' ', x) for x in ...]` | Applies the cleaning function to each speech text to create a uniform, cleaned list of texts. |\n",
    "| **5. Full Cleaning Pipeline** | `text = [re.sub(r'[^A-Za-z0-9]+', ' ', x) for x in [inaugural.raw(file_id) for file_id in inaugural.fileids()]]` | Combines the file reading and cleaning into one list comprehension to produce a cleaned list of inaugural addresses. |\n",
    "| **6. Lowercase Conversion** | `map(str.lower, text)` | Applies the `lower()` function to each text to convert all characters to lowercase, making text processing case-insensitive. |\n",
    "| **7. Convert to List** | `list(map(str.lower, text))` | Converts the map object into a list of lowercase texts. |\n",
    "| **8. Final Assignment** | `text = list(map(str.lower, text))` | Stores the fully cleaned and normalized texts back in the `text` variable for further analysis. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [re.sub(r'[^A-Za-z0-9]+', ' ', x) for x in [inaugural.raw(file_id) for file_id in inaugural.fileids()]]\n",
    "\n",
    "text = list(map(str.lower, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7561a1f",
   "metadata": {},
   "source": [
    "Note: `re.sub(r'[\\W]+', ' ', x)` or `re.sub(r'[^\\w]+', ' ', x)` should also work in this case. The only difference is that these methods will also replace the underscore `'_'` with space.\n",
    "- \\w: word\n",
    "- \\W: non word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440555f8",
   "metadata": {},
   "source": [
    "Now let's take a look at the cleaned data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the cleaned version of text data:\n",
    "text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9ffd4",
   "metadata": {},
   "source": [
    "Now, with the data cleaned, let's perform text analysis with TF-IDF!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43d245-1277-4ea0-8e22-85bdd6f8ddf2",
   "metadata": {},
   "source": [
    "#### What is TF-IDF?\n",
    "\n",
    "**TF-IDF** stands for **Term Frequency–Inverse Document Frequency**. It is a numerical statistic that reflects how important a word is to a document in a collection (or corpus).\n",
    "\n",
    "TF-IDF is commonly used in **text mining**, **information retrieval**, and **natural language processing** to convert textual data into meaningful numerical features.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "- **Term Frequency (TF)** measures how often a term appears in a document.\n",
    "- **Inverse Document Frequency (IDF)** measures how unique or rare a term is across all documents.\n",
    "- The idea is to **increase the weight** of terms that occur frequently in a document **but not in many other documents** (i.e., not too common).\n",
    "\n",
    "#### Formula\n",
    "\n",
    "Let:\n",
    "- $ t $: a term (word)\n",
    "- $ d $: a single document\n",
    "- $ D $: the full corpus (set of documents)\n",
    "\n",
    "#### Term Frequency (TF):\n",
    "$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times } t \\text{ appears in } d}{\\text{Total number of terms in } d}\n",
    "$\n",
    "\n",
    "#### Inverse Document Frequency (IDF):\n",
    "$\n",
    "\\text{IDF}(t, D) = \\log\\left(\\frac{N}{1 + n_t}\\right)\n",
    "$\n",
    "- $ N $: total number of documents in the corpus\n",
    "- $ n_t $: number of documents where the term \\( t \\) appears\n",
    "- (The `+1` avoids division by zero)\n",
    "\n",
    "#### TF-IDF Score:\n",
    "$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$\n",
    "\n",
    "#### Why Use TF-IDF?\n",
    "\n",
    "- Helps filter out **common words** (e.g., \"the\", \"and\", \"is\") that are less meaningful for distinguishing documents.\n",
    "- Highlights **discriminative terms** that are more relevant or unique to individual documents.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Use Case\n",
    "\n",
    "If the word *\"freedom\"* appears frequently in one president’s inaugural address but rarely in others, TF-IDF will assign it a **higher score** for that speech—indicating its importance in that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451aa0a",
   "metadata": {},
   "source": [
    "To get started on your TF-IDF analysis, you'll first want to **create a `TfidfVectorizer` object to transform your text data into vectors. Assign this `TfidfVectorizer` object to `tfidf`.**\n",
    "\n",
    "In this object, you'll need to **pass five arguments to initialize a `TfidfVectorizer`**: \n",
    "* set to apply TF scaling: `sublinear_tf=True`\n",
    "* analyze at the word-level: `analyzer='word'`\n",
    "* set maximum number of unique words: ` max_features=2000`\n",
    "* specify that you want to tokenize the data using the word_tokenizer from NLTK: `tokenizer=word_tokenize`\n",
    "* remove English language stop words: `stop_words=stopwords.words(\"english\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f5179",
   "metadata": {},
   "source": [
    "Note: Stop words are a set of commonly used words in a language,which are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                        analyzer='word',\n",
    "                        max_features=2000,\n",
    "                        tokenizer=word_tokenize,\n",
    "                        stop_words=stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4100e4-b10b-4c3a-a038-c726fa98b8ed",
   "metadata": {},
   "source": [
    "The `TfidfVectorizer` will transform raw text documents into a matrix where each row represents a document and each column represents one of the top 2000 words. The value in each cell is the TF-IDF score, which is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d3c53",
   "metadata": {},
   "source": [
    "Now, it's time to calculate TF-IDF for words across our corpus of Inaugural addresses! \n",
    "\n",
    "To do this:\n",
    "\n",
    "1. Use the `tfidf.fit_transform` function to calculate TF-IDF on your `text` variable. Use `.toarray()` to transform the output to a numpy array named `tfidf_array`. \n",
    "2. Generate a DataFrame `inaug_tfidf` by passing `tfidf_array` into `pd.DataFrame`.\n",
    "2. Be sure that the columns are named with the columns of the words the values represent and your index in `inaug_tfidf` is the year of the address. The `get_feature_names` method from `tfidf` may help you accomplish the columns name assignment. And the `years` you created earlier may help you with the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the preprocessed text data into a TF-IDF matrix and convert it to a pandas DataFrame.\n",
    "# 1. tfidf.fit_transform(text): This line applies the previously initialized TfidfVectorizer to the text data.\n",
    "#    - 'fit_transform' function computes the TF-IDF scores for each word in the text data.\n",
    "#    - The result is a sparse matrix where each row corresponds to a document and each column represents \n",
    "# a term from the TF-IDF vocabulary.\n",
    "\n",
    "inaug_tfidf = pd.DataFrame(tfidf.fit_transform(text).toarray())\n",
    "\n",
    "# 2. Convert the sparse matrix to a dense matrix using '.toarray()', and then into a DataFrame.\n",
    "#    - The DataFrame 'inaug_tfidf' represents the TF-IDF features of the text data, where each row is \n",
    "# a document and each column is a TF-IDF feature.\n",
    "\n",
    "inaug_tfidf.columns = tfidf.get_feature_names_out()\n",
    "\n",
    "# 3. Assign column names to the DataFrame using 'tfidf.get_feature_names_out()'.\n",
    "#    - Each column in the DataFrame is named after a term in the TF-IDF vocabulary.\n",
    "\n",
    "inaug_tfidf.index = years\n",
    "\n",
    "# 4. Set the DataFrame index using 'years'.\n",
    "#    - This implies that 'years' is an iterable (probably a list or array) that contains the years \n",
    "# associated with each text document.\n",
    "#    - Each row in the DataFrame is now associated with a specific year, making it easier to reference \n",
    "# the documents chronologically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0011d",
   "metadata": {},
   "source": [
    "Let's then take a look at the dataframe we just generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003fe77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inaug_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24dc33",
   "metadata": {},
   "source": [
    "Now, let's extract the single most unique word from each address and save it in a variable called `most_unique`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e66ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most uniquely significant word for each document in the TF-IDF matrix.\n",
    "# 1. inaug_tfidf.idxmax(axis=1): This method is applied to the 'inaug_tfidf' DataFrame.\n",
    "#    - 'idxmax' function finds the index (column name in this case) of the first occurrence \n",
    "# of the maximum value along the specified axis.\n",
    "#    - 'axis=1' specifies that the operation is to be performed along the columns for each row.\n",
    "#    - For each row (document) in the DataFrame, 'idxmax' identifies the column (word) that has \n",
    "# the highest TF-IDF score.\n",
    "# 2. The result 'most_unique' is a pandas Series where the index corresponds to the indices of \n",
    "# documents in 'inaug_tfidf', \n",
    "#    and the values are the names of the words (columns in 'inaug_tfidf') that have the highest \n",
    "# TF-IDF score in each document.\n",
    "#    - Essentially, this series maps each document to the word that is most uniquely significant \n",
    "# to that document based on TF-IDF scores.\n",
    "\n",
    "\n",
    "most_unique = inaug_tfidf.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117efe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b86c8",
   "metadata": {},
   "source": [
    "Now let's plot the TF-IDF value for `\"british\"`, `\"america\"`, `\"war\"`, and `\"jobs\"` on the y-axis and plot the years on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd656cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting TF-IDF scores for specific words across different years.\n",
    "# 'x' is a list or array representing years corresponding to each document.\n",
    "x = inaug_tfidf.index\n",
    "\n",
    "plt.plot(x, inaug_tfidf['british'], label=\"british\")\n",
    "# Plot the TF-IDF scores of the word \"british\" across different years.\n",
    "\n",
    "plt.plot(x, inaug_tfidf['america'], label=\"america\")\n",
    "# Plot the TF-IDF scores of the word \"america\" across different years.\n",
    "\n",
    "plt.plot(x, inaug_tfidf['war'], label=\"war\")\n",
    "# Plot the TF-IDF scores of the word \"war\" across different years.\n",
    "\n",
    "plt.plot(x, inaug_tfidf['jobs'], label=\"jobs\")\n",
    "# Plot the TF-IDF scores of the word \"jobs\" across different years.\n",
    "\n",
    "# Setting the labels for the x and y axes.\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('TF-IDF')\n",
    "\n",
    "# Adding a legend to the upper right corner of the plot.\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Setting the x-axis ticks. np.arange(0, 56, step=5) suggests the range starts at 0, \n",
    "# ends before 56, with a step of 5 for better visualization.\n",
    "plt.xticks(np.arange(0, 56, step=5))\n",
    "\n",
    "# Display the plot.\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35e022",
   "metadata": {},
   "source": [
    "What kind of trend can you see from the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002bcbfe",
   "metadata": {},
   "source": [
    "Now you should be able to finish D7!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
